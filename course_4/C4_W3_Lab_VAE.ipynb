{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "C4_W3_Lab_VAE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPk8pQ/OyzAbW6eG2TbTtgF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "afbec413727f4b9d8e7eca98a0e56232": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e61785dbdd174eab98ec8e79557cd5fc",
              "IPY_MODEL_f4004a8dab78405f8c7eab897b70e7f8",
              "IPY_MODEL_f136d09b036a443282d5480d94ba463d"
            ],
            "layout": "IPY_MODEL_476e84229f174b92883198a64ebcc047"
          }
        },
        "e61785dbdd174eab98ec8e79557cd5fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c9bdb86869d427692b7a9e32172992e",
            "placeholder": "​",
            "style": "IPY_MODEL_2ca972244d2a403c81ea769126c92ecb",
            "value": "Dl Completed...: 100%"
          }
        },
        "f4004a8dab78405f8c7eab897b70e7f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61f0a588c8f046389a246c00c5e24cee",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b15cf34929e44882a2fd465c18923dac",
            "value": 4
          }
        },
        "f136d09b036a443282d5480d94ba463d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66d9bf59f10e4d8180dc611272d42313",
            "placeholder": "​",
            "style": "IPY_MODEL_392ea67480cb445287addf246434c4d4",
            "value": " 4/4 [00:00&lt;00:00,  6.26 file/s]"
          }
        },
        "476e84229f174b92883198a64ebcc047": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c9bdb86869d427692b7a9e32172992e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ca972244d2a403c81ea769126c92ecb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61f0a588c8f046389a246c00c5e24cee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b15cf34929e44882a2fd465c18923dac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "66d9bf59f10e4d8180dc611272d42313": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "392ea67480cb445287addf246434c4d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ksairos/tf-advanced-techniques-course/blob/main/course_4/C4_W3_Lab_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variational AutoEncoder"
      ],
      "metadata": {
        "id": "YNNEothgohei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Variational AutoEncoder on MNIST dataset to generate new images of digits"
      ],
      "metadata": {
        "id": "azbXHTRrooiS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports|"
      ],
      "metadata": {
        "id": "voABFfGXoxoI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "_7G6_lztoXr3"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "K = tf.keras.backend"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameters"
      ],
      "metadata": {
        "id": "3UTM6KHMozoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "LATENT_DIM = 2"
      ],
      "metadata": {
        "id": "KNafECwsog84"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Dataset"
      ],
      "metadata": {
        "id": "EWS2jH2Oo529"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilities\n",
        "\n",
        "def map_image(image, label):\n",
        "  '''returns a normalized and reshaped tensor from a given image'''\n",
        "  image = tf.cast(image, dtype=tf.float32)\n",
        "  image = image / 255.0\n",
        "  image = tf.reshape(image, shape=(28, 28, 1,))\n",
        "  \n",
        "  return image\n",
        "\n",
        "\n",
        "def get_dataset(map_fn, is_validation=False):\n",
        "  '''Loads and prepares the mnist dataset from TFDS.'''\n",
        "  if is_validation:\n",
        "    split_name = \"test\"\n",
        "  else:\n",
        "    split_name = \"train\"\n",
        "\n",
        "  dataset = tfds.load('mnist', as_supervised=True, split=split_name)\n",
        "  dataset = dataset.map(map_fn)\n",
        "  \n",
        "  if is_validation:\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "  else:\n",
        "    dataset = dataset.shuffle(1024).batch(BATCH_SIZE)\n",
        "\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "-a7k7lxFo48f"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = get_dataset(map_image)\n",
        "valid_dataset = get_dataset(map_image, is_validation=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217,
          "referenced_widgets": [
            "afbec413727f4b9d8e7eca98a0e56232",
            "e61785dbdd174eab98ec8e79557cd5fc",
            "f4004a8dab78405f8c7eab897b70e7f8",
            "f136d09b036a443282d5480d94ba463d",
            "476e84229f174b92883198a64ebcc047",
            "7c9bdb86869d427692b7a9e32172992e",
            "2ca972244d2a403c81ea769126c92ecb",
            "61f0a588c8f046389a246c00c5e24cee",
            "b15cf34929e44882a2fd465c18923dac",
            "66d9bf59f10e4d8180dc611272d42313",
            "392ea67480cb445287addf246434c4d4"
          ]
        },
        "id": "C4cjqPTkpBML",
        "outputId": "b2cf3275-4424-48e8-8e70-b5903a7e3bf4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDownloading and preparing dataset mnist/3.0.1 (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /root/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n",
            "local data directory. If you'd instead prefer to read directly from our public\n",
            "GCS bucket (recommended if you're running on GCP), you can instead pass\n",
            "`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Completed...:   0%|          | 0/4 [00:00<?, ? file/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "afbec413727f4b9d8e7eca98a0e56232"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the model"
      ],
      "metadata": {
        "id": "2eq-nrmYpVDm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1YAZAeMGEJ1KgieYk1ju-S9DoshpMREeC\" width=\"60%\" height=\"60%\"/>"
      ],
      "metadata": {
        "id": "P2veDGaLp7BU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Architecture of VAE is similar to general auto-encoders with main difference in latent representation. It takes output of the encoder and mixes it with a random sample, letting us generate something new."
      ],
      "metadata": {
        "id": "5t1yUzjyqiI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sampling Class"
      ],
      "metadata": {
        "id": "jgKa1e2YqRWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom layer that defines a gray box in the diagram"
      ],
      "metadata": {
        "id": "-jiOswL1qu9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sampling(tf.keras.layers.Layer):\n",
        "  def call(self, input):\n",
        "    \"\"\"Generates a random sample and combines with the encoder output\n",
        "\n",
        "    Args:\n",
        "      inputs -- output tensor from the encoder\n",
        "\n",
        "    Returns:\n",
        "      `inputs` tensors combined with a random sample\n",
        "    \"\"\"\n",
        "\n",
        "    # unpack the output of the encoder \n",
        "    mu, sigma = input\n",
        "\n",
        "    # get the size and dimentions of the batch\n",
        "    batch = tf.shape(mu)[0]\n",
        "    dim = tf.shape(mu)[1]\n",
        "\n",
        "    # generate a random tensor\n",
        "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "\n",
        "    # mix the inputs and generated noise\n",
        "    return mu + tf.exp(0.5 * sigma) * epsilon"
      ],
      "metadata": {
        "id": "dGpPr_plpPpS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "h3OFjcuKtW_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1eoxFK_UVSHd3a_5EHcCU8F8QDZlPiXfW\" width=\"60%\" height=\"60%\"/>"
      ],
      "metadata": {
        "id": "4UlLX6j9taPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense, Conv2D, BatchNormalization, Flatten, Input, Reshape, Conv2DTranspose\n",
        "\n",
        "def encoder_layers(inputs, latent_dim):\n",
        "  \"\"\"Defines the encoder's layers.\n",
        "  Args:\n",
        "    inputs -- batch from the dataset\n",
        "    latent_dim -- dimensionality of the latent space\n",
        "\n",
        "  Returns:\n",
        "    mu -- learned mean\n",
        "    sigma -- learned standard deviation\n",
        "    batch_2.shape -- shape of the features before flattening\n",
        "  \"\"\" \n",
        "\n",
        "  # Conv2D layers followed by BatchNormalization layers\n",
        "  x = Conv2D(filters=32, kernel_size=3, strides=2, padding='same', activation='relu', name='encoder_conv1')(inputs)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Conv2D(filters=64, kernel_size=3, strides=2, padding='same', activation='relu', name='encoder_conv2')(x)\n",
        "\n",
        "  # this will be our third output\n",
        "  batch_2 = BatchNormalization()(x)\n",
        "\n",
        "  # Flatten before feeding to Dense layer\n",
        "  x = Flatten(name=\"encoder_flatten\")(batch_2)\n",
        "\n",
        "  # Dense layer\n",
        "  x = Dense(20, activation='relu', name='encoder_dense')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  # Dense layers for mu and sigma with latent_dim units\n",
        "  mu = Dense(latent_dim, name='latent_mu')(x)\n",
        "  sigma = Dense(latent_dim, name='latent_sigma')(x)\n",
        "\n",
        "  return mu, sigma, batch_2.shape"
      ],
      "metadata": {
        "id": "He_iqqCDtV72"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder_model(latent_dim, input_shape):\n",
        "  \"\"\"Defines the encoder model with the Sampling layer\n",
        "  Args:\n",
        "    latent_dim -- dimensionality of the latent space\n",
        "    input_shape -- shape of the dataset batch\n",
        "\n",
        "  Returns:\n",
        "    model -- the encoder model\n",
        "    conv_shape -- shape of the features before flattening\n",
        "  \"\"\"\n",
        "\n",
        "  # input layer\n",
        "  inputs = Input(shape=input_shape)\n",
        "\n",
        "  # get the output of encoder layers\n",
        "  mu, sigma, conv_shape = encoder_layers(inputs, latent_dim)\n",
        "\n",
        "  # feed mu and sigma to Sampling layer\n",
        "  z = Sampling()((mu, sigma))\n",
        "\n",
        "  # build encoder model\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=[mu, sigma, z])\n",
        "\n",
        "  return model, conv_shape"
      ],
      "metadata": {
        "id": "PQX8fIZJvzem"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K.clear_session()\n",
        "\n",
        "tmp_encoder, _ = encoder_model(LATENT_DIM, (28, 28, 1))\n",
        "tmp_encoder.summary()\n",
        "\n",
        "del tmp_encoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itq10ArAIDzt",
        "outputId": "f0d77d8f-507d-42e9-c921-dd51acb6418c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " encoder_conv1 (Conv2D)         (None, 14, 14, 32)   320         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 14, 14, 32)  128         ['encoder_conv1[0][0]']          \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " encoder_conv2 (Conv2D)         (None, 7, 7, 64)     18496       ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 7, 7, 64)    256         ['encoder_conv2[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " encoder_flatten (Flatten)      (None, 3136)         0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " encoder_dense (Dense)          (None, 20)           62740       ['encoder_flatten[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 20)          80          ['encoder_dense[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " latent_mu (Dense)              (None, 2)            42          ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " latent_sigma (Dense)           (None, 2)            42          ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " sampling (Sampling)            (None, 2)            0           ['latent_mu[0][0]',              \n",
            "                                                                  'latent_sigma[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 82,104\n",
            "Trainable params: 81,872\n",
            "Non-trainable params: 232\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder\n",
        "This part of the model expands latent representations back to original size image. It will resemble training data"
      ],
      "metadata": {
        "id": "uTIPB_FVHGYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder_layers(inputs, conv_shape):\n",
        "  \"\"\"Defines the decoder layers.\n",
        "  Args:\n",
        "    inputs -- output of the encoder \n",
        "    conv_shape -- shape of the features before flattening\n",
        "\n",
        "  Returns:\n",
        "    tensor containing the decoded output\n",
        "  \"\"\"\n",
        "\n",
        "  # Compute number of units from conv_shape dimentions and create a Dense layer\n",
        "  units = conv_shape[1] * conv_shape[2] * conv_shape[3]\n",
        "  x = Dense(units, activation='relu', name='decoder_dense')(inputs)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  # Reshape resulting dense layer into conv_shape\n",
        "  x = Reshape((conv_shape[1], conv_shape[2], conv_shape[3]), name=\"decoder_reshape\")(x)\n",
        "\n",
        "  # Upsample back to original size with deconvolutional layers\n",
        "  x = Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='same', activation='relu', name='decoder_deconv2d_1')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Conv2DTranspose(filters=32, kernel_size=3, strides=2, padding='same', activation='relu', name='decoder_deconv2d_2')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Conv2DTranspose(filters=1, kernel_size=3, strides=1, padding='same', activation='relu', name='decoder_deconv2d_final')(x)\n",
        "\n",
        "  return x"
      ],
      "metadata": {
        "id": "YXMNhduFw5vc"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder_model(latent_dim, conv_shape):\n",
        "  \"\"\"Defines the decoder model.\n",
        "  Args:\n",
        "    latent_dim -- dimensionality of the latent space\n",
        "    conv_shape -- shape of the features before flattening\n",
        "\n",
        "  Returns:\n",
        "    model -- the decoder model\n",
        "  \"\"\"\n",
        "\n",
        "  inputs = Input(shape=(latent_dim, ))\n",
        "\n",
        "  outputs = decoder_layers(inputs, conv_shape)\n",
        "\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "ZrMzkVLoMHRw"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K.clear_session()\n",
        "\n",
        "tmp_decoder = decoder_model(LATENT_DIM, (None, 7, 7, 64))\n",
        "tmp_decoder.summary()\n",
        "\n",
        "del tmp_decoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQqFoHahNE2j",
        "outputId": "5db6f3c5-be73-47d3-ebb3-64492f25002c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 2)]               0         \n",
            "                                                                 \n",
            " decoder_dense (Dense)       (None, 3136)              9408      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 3136)             12544     \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " decoder_reshape (Reshape)   (None, 7, 7, 64)          0         \n",
            "                                                                 \n",
            " decoder_deconv2d_1 (Conv2DT  (None, 14, 14, 64)       36928     \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 14, 14, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " decoder_deconv2d_2 (Conv2DT  (None, 28, 28, 32)       18464     \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 28, 28, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " decoder_deconv2d_final (Con  (None, 28, 28, 1)        289       \n",
            " v2DTranspose)                                                   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 78,017\n",
            "Trainable params: 71,553\n",
            "Non-trainable params: 6,464\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Kullback–Leibler Divergence\n",
        "We add Kullback–Leibler Divergence to our reconstruction loss in order to improve generative capabilities of our model"
      ],
      "metadata": {
        "id": "ntJHNecHSF-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kl_reconstruction_loss(inputs, outputs, mu, sigma):\n",
        "  \"\"\" Computes the Kullback-Leibler Divergence (KLD)\n",
        "  Args:\n",
        "    inputs -- batch from the dataset\n",
        "    outputs -- output of the Sampling layer\n",
        "    mu -- mean\n",
        "    sigma -- standard deviation\n",
        "\n",
        "  Returns:\n",
        "    KLD loss\n",
        "  \"\"\"\n",
        "  kl_loss = 1 + sigma - tf.square(mu) - tf.math.exp(sigma)\n",
        "  kl_loss = tf.reduce_mean(kl_loss) * -0.5\n",
        "  \n",
        "  return kl_loss"
      ],
      "metadata": {
        "id": "jDwXztAzNOd2"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Entire VAE Model"
      ],
      "metadata": {
        "id": "fwHAMn6zSyeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vae_model(encoder, decoder, input_shape):\n",
        "  \"\"\"Defines the VAE model\n",
        "  Args:\n",
        "    encoder -- the encoder model\n",
        "    decoder -- the decoder model\n",
        "    input_shape -- shape of the dataset batch\n",
        "\n",
        "  Returns:\n",
        "    the complete VAE model\n",
        "  \"\"\"\n",
        "\n",
        "  # input\n",
        "  inputs = Input(input_shape)\n",
        "  \n",
        "  # encoder outputs\n",
        "  mu, sigma, z = encoder(inputs)\n",
        "\n",
        "  # reconstructed output from the decoder   \n",
        "  reconstructed = decoder(z)\n",
        "\n",
        "  # define the model\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=reconstructed)\n",
        "\n",
        "  # add the KL loss \n",
        "  loss = kl_reconstruction_loss(inputs, z, mu, sigma)\n",
        "  model.add_loss(loss)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "_izc55SmSphE"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the function that combines everything we were writing"
      ],
      "metadata": {
        "id": "-yWfHCCzUgy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_models(input_shape, latent_dim):\n",
        "  \"\"\"Returns the encoder, decoder, and vae models\"\"\"\n",
        "  encoder, conv_shape = encoder_model(latent_dim, input_shape)\n",
        "  decoder = decoder_model(latent_dim, conv_shape)\n",
        "  vae = vae_model(encoder, decoder, input_shape)\n",
        "  \n",
        "  return encoder, decoder, vae"
      ],
      "metadata": {
        "id": "YmDBe9oMUa7S"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K.clear_session()\n",
        "\n",
        "encoder, decoder, vae = get_models(input_shape=(28,28,1), latent_dim=LATENT_DIM)"
      ],
      "metadata": {
        "id": "mi1RBx4UVFud"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vae.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mqDi2TLVLrh",
        "outputId": "311424e8-8295-4b34-9cbc-dbca46b1533b"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " model (Functional)             [(None, 2),          82104       ['input_3[0][0]']                \n",
            "                                 (None, 2),                                                       \n",
            "                                 (None, 2)]                                                       \n",
            "                                                                                                  \n",
            " model_1 (Functional)           (None, 28, 28, 1)    78017       ['model[0][2]']                  \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOpLamb  (None, 2)           0           ['model[0][1]']                  \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " tf.math.square (TFOpLambda)    (None, 2)            0           ['model[0][0]']                  \n",
            "                                                                                                  \n",
            " tf.math.subtract (TFOpLambda)  (None, 2)            0           ['tf.__operators__.add[0][0]',   \n",
            "                                                                  'tf.math.square[0][0]']         \n",
            "                                                                                                  \n",
            " tf.math.exp (TFOpLambda)       (None, 2)            0           ['model[0][1]']                  \n",
            "                                                                                                  \n",
            " tf.math.subtract_1 (TFOpLambda  (None, 2)           0           ['tf.math.subtract[0][0]',       \n",
            " )                                                                'tf.math.exp[0][0]']            \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean (TFOpLambd  ()                  0           ['tf.math.subtract_1[0][0]']     \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.math.multiply (TFOpLambda)  ()                   0           ['tf.math.reduce_mean[0][0]']    \n",
            "                                                                                                  \n",
            " add_loss (AddLoss)             ()                   0           ['tf.math.multiply[0][0]']       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 160,121\n",
            "Trainable params: 153,425\n",
            "Non-trainable params: 6,696\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ],
      "metadata": {
        "id": "JgFrNXzOXKqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model parameters\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_metric = tf.keras.metrics.Mean()\n",
        "bce_loss = tf.keras.losses.BinaryCrossentropy()"
      ],
      "metadata": {
        "id": "JWjRkpxwWCwT"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper function to see the progress of the model at the end of every epoch"
      ],
      "metadata": {
        "id": "FqYKdJqFXhUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_save_images(model, epoch, step, test_input):\n",
        "  \"\"\"Helper function to plot our 16 images\n",
        "\n",
        "  Args:\n",
        "\n",
        "  model -- the decoder model\n",
        "  epoch -- current epoch number during training\n",
        "  step -- current step number during training\n",
        "  test_input -- random tensor with shape (16, LATENT_DIM)\n",
        "  \"\"\"\n",
        "\n",
        "  # generate images from the test input\n",
        "  predictions = model.predict(test_input)\n",
        "\n",
        "  # plot the results\n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  # tight_layout minimizes the overlap between 2 sub-plots\n",
        "  fig.suptitle(\"epoch: {}, step: {}\".format(epoch, step))\n",
        "  plt.savefig('image_at_epoch_{:04d}_step{:04d}.png'.format(epoch, step))\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "oUGITdayXYTs"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop"
      ],
      "metadata": {
        "id": "H2FH49DcX0Zs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate random vector as test input to the decoder\n",
        "random_vector_for_generation = tf.random.normal(shape=[16, LATENT_DIM])\n",
        "\n",
        "# number of epochs\n",
        "epochs = 100\n",
        "\n",
        "# initialize the helper function to display outputs from an untrained model\n",
        "generate_and_save_images(decoder, 0, 0, random_vector_for_generation)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print('Start of epoch %d' % (epoch,))\n",
        "\n",
        "  # iterate over batches\n",
        "  for step, x_batch_train in enumerate(train_dataset):\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "      # feed a batch to the VAE model\n",
        "      reconstructed = vae(x_batch_train)\n",
        "\n",
        "      # compute reconstruction loss\n",
        "      flattened_inputs = tf.reshape(x_batch_train, shape=[-1])\n",
        "      flattened_outputs = tf.reshape(reconstructed, shape=[-1])\n",
        "      loss = bce_loss(flattened_inputs, flattened_outputs) * 784\n",
        "\n",
        "      # add KLD regularization loss\n",
        "      loss += sum(vae.losses)\n",
        "\n",
        "    # get gradient and update weights\n",
        "    grads = tape.gradient(loss, vae.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
        "\n",
        "    # compute the loss metric\n",
        "    loss_metric(loss)\n",
        "\n",
        "    # display outputs every 100 steps\n",
        "    if step % 100 == 0:\n",
        "      display.clear_output(wait=False)\n",
        "      generate_and_save_images(decoder, epoch, step, random_vector_for_generation)\n",
        "      print('Epoch: %s step: %s mean loss = %s' % (epoch, step, loss_metric.result().numpy()))"
      ],
      "metadata": {
        "id": "vorvWZN9XtRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xxib0n0jYWFv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}